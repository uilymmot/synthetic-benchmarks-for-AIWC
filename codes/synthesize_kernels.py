import clgen.dbutil

#The data-base was generated by mining github for OpenCL kernel files as training samples.
#10k files were used to generate this training corpus.
#These can be reproduced using clgen-create-db:
#
# Generate an API token by following the instructions here:
#   https://help.github.com/articles/creating-an-access-token-for-command-line-use/>
# 
# From the command line, run:
#
#   $ export GITHUB_USERNAME=[your-username]
#   $ export GITHUB_PW=[your-password]
#   $ export GITHUB_TOKEN=[your-api-token]
#   $ clgen-create-db --github github.db
#   $ clgen-fetch-github github.db
#
# instructions to generate the github api token can be found [here]<https://help.github.com/articles/creating-an-access-token-for-command-line-use>

data_base = "./github.db"
db = clgen.dbutil.connect(data_base).cursor()

#to sample the data-base:
db.execute("SELECT Contents FROM ContentFiles ORDER BY RANDOM() LIMIT 1")
print(db.fetchone()[0])

#failures caused by undeclared identifiers, the same shim file from Cummins et. al is used as header which contains inferred values for common type definitions
with open(clgen.native.SHIMFILE) as f: shimfile = f.read()

#code rewriting process -- remove macros, conditional compilation, and source comments, Identifiers are rewritten to have a short but unique name based on their order of appearance and code style is enforced to ensure consistent use of braces, parentheses, and white space
#clgen.preprocess.preprocess_db("./github.db")
#this took 165 hours to process all kernels on github

import clgen.train

db = clgen.dbutil.connect("./github.db")
clgen.train.create_corpus(db, "./corpus.cl", gh=True)

def line_word_char_count(path):
    """count words, lines, chars in file"""
    num_lines = 0
    num_words = 0
    num_chars = 0

    with open(path) as infile:
        for line in infile:
            words = line.split()

            num_lines += 1
            num_words += len(words)
            num_chars += len(line)

    return num_lines, num_words, num_chars

lc, wc, cc = line_word_char_count("./corpus.cl")
print("  #. lines:", lc)
print("  #. words:", wc)
print("  #. chars:", cc)

#output the training corpus as a directory
clgen.train.create_corpus(db, "./corpus", gh=True,dir=True)

#build a model
synth_mod = clgen.model.from_json({
    "corpus":{"path":'./corpus'},
    "train_opts": {
        "model_type": "lstm",
        "rnn_size": 128,
        "num_layers": 2,
        "max_epochs": 1
        }
    })

synth_mod.cache.empty()
synth_mod.train()

